\documentclass[12pt]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[dvipsnames, table]{xcolor}
\colorlet{DarkRed}{Red!90!black}
\colorlet{LightRed}{Red!10!white}
\colorlet{DarkGreen}{Green!50!black}
\colorlet{LightGreen}{Green!10!white}
\usepackage{colortbl} % https://texblog.org/2011/04/19/highlight-table-rowscolumns-with-color/

% sections
\usepackage{titlesec} % https://latex.org/forum/viewtopic.php?t=10456
\titleformat*{\section}{\large\bfseries}
\titlespacing*{\section}{0pt}{2mm}{2mm}
\titleformat{\subsection}[runin]% runin puts it in the same paragraph
{\normalfont\bfseries}% formatting commands to apply to the whole heading
{\thesubsection}% the label and number
{0.5em}% space between label/number and subsection title
{}% formatting commands applied just to subsection title
[.]% punctuation or other commands following subsection title
%\titleformat{\section}[runin]{\normalfont\bfseries}{\thesection}{0.5em}{}[.]
\titleformat{\subsubsection}[runin]{\normalfont\bfseries}{\thesubsubsection}{0.5em}{}[.]

% links
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	linkcolor={DarkRed},
	citecolor={DarkRed},
	urlcolor={blue}
}

\usepackage{geometry}
\newgeometry{
	left=2cm, right=1.5cm, top=1.5cm, bottom=1.5cm,
	includefoot, heightrounded
}

\usepackage[parfill]{parskip} % https://tex.stackexchange.com/a/16703/135296

% sub figures / grids of pictures
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{{img/}} % includegraphics path
% \usepackage[export]{adjustbox} % https://tex.stackexchange.com/questions/20640/how-to-add-border-for-an-image
\newcommand{\includegraphicsw}[2][1.]{\includegraphics[width=#1\linewidth]{#2}}
\newcommand{\svginput}[1]{\input{img/#1}} % pdf_tex path
\newcommand{\svginputw}[2][\linewidth]{\def\svgwidth{#1}\input{img/#2}} % pdf_tex path

% tables
\usepackage{longtable}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{float} % for H

% bold for everything
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% differentials
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Dist}{dist}
\newcommand{\sphere}{{\Gamma_{\text{sph}}}}
\newcommand{\tor}{{\Gamma_{\text{tor}}}}

\newcommand{\HOne}{{\mathbb H^1}}
\newcommand{\LTwo}{{\mathbb L^2}}
\newcommand{\LTwoSpace}[1][\Gamma]{{\mathbb L^2\left({#1}\right)}}
\newcommand{\HOneSpace}[1][\Gamma]{{\mathbb H^1\left({#1}\right)}}

\newcommand{\cl}[1]{\texttt{\$~#1}}

\usepackage{listings}
%\definecolor{mygreen}{rgb}{0,0.6,0}
\lstset{
%	language=C++,
%	basicstyle=\footnotesize\ttfamily,
	breaklines=true,
%	commentstyle=\color{mygreen},
	frame=l,
	xleftmargin=5pt,
	tabsize=2,
%	belowskip=-1pt
} 

% https://tex.stackexchange.com/questions/9425/how-to-fix-footnote-position-at-the-bottom-of-the-page
\usepackage[bottom]{footmisc}
\newcommand{\AZ}[1]{{\color{red}\textbf{AZ}:~#1}}
\newcommand{\KW}[1]{{\color{blue}\textbf{KW}:~#1}}
\newcommand{\dimSize}{n}

\title{COSC\,6365 -- Introduction to High Performance Computing, Fall 2019\\Final project: \textbf{Matrix-free finite element method}}
\author{
	Kyle Williams\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (kylew@math.uh.edu).} \and
	Alexander Zhiliakov\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (alex@math.uh.edu).}
}

\begin{document}
	
\maketitle

\tableofcontents
\vfill
\clearpage
\let\oldtabular\tabular
\renewcommand{\tabular}[1][1.5]{\def\arraystretch{#1}\oldtabular}
\renewcommand\arraystretch{1.3}

\section{Theoretical background}

\subsection{Finite element method}
The Finite Element Method is a general method for solving partial differential equations. So we begin with a partial differential equation
\[
\begin{cases} 
	\text{Find } u \in V(\Omega) \text{ such that } \\
	\langle \mathcal{L}u, v \rangle = \langle f, v \rangle \; \forall v \in V(\Omega)
\end{cases}
\]

Where $\Omega$ is the domain on which we wish to solve the differential equation, and $V$ is an appropriate vector space depending on the particular differential equation and boundary conditions in quesiton. Generally this vector space will be infinite dimensional and will require some sort of approximation if we wish to solve the problem numerically. In the finite element method, we use the Ritz-Galerkin approach to approximation. Instead of choosing an infinite dimensional vectorspace, we choose a finite dimensional subspace $V_{h}$ so the problem becomes
\[
\begin{cases} 
	\text{Find } u_{h} \in V_{h}(\Omega) \text{ such that } \\
	\langle \mathcal{L}u_{h}, v_{h} \rangle = \langle f, v_{h} \rangle \; \forall v_{h} \in V_{h}(\Omega)
\end{cases}
\]
Since $V_{h}$ is a finite dimensional, we case choose basis functions for the vector space $\varphi_{i} \; i = 1, ..., N$, with this we can represent $u_{h} = \sum_{i = 1}^{N} U_{i}\varphi_{i}$, additionally we only need to test that the equallity holds for each basis function which leads to
\[
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{N} \text{ such that } \\
	\sum_{i = 1}^{N}\langle \mathcal{L}U_{i}\varphi_{i}, \varphi_{j} \rangle = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots, N
\end{cases}
\]
If the differential equation is linear this becomes a linear system
\[
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{N} \text{ such that } \\
	\sum_{i = 1}^{N}\langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle U_{i} = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots, N
\end{cases}
\]
We can represent this linear system with $\mathbf{A}_{i,j} = \langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle,\; x_{i} = U_{i}$ and $\mathbf{b}_{i} = \langle f, \varphi_{i} \rangle$ for the right hand side. The finite element method is a method for choosing these basis functions in such a way that the resulting system will have certain specified properties, e.g. $\mathbf{A}$ is sparse.

In the finite element method to choose these basis functions, we first take the domain $\Omega$ and triangulate it into a collection of polyhedral or quadralateral cells (or elements). On each of these cells, we assign a set of points $p$ called degrees of freedom (dofs); the basis functions are then chosen so that $\varphi_{i}(p_{j}) = \delta_{i}^{j}$. This will have the effect of each basis function being a polynomial of degree $k$ where $k$ is the number of degrees of freedom per cell.

With this particualar choice, for cells that do not contain, say, the dof $p_{m}$, the corresponding basis function $\varphi_{m}$ will also be zero. This means that $\langle \mathcal{L}\varphi_{i}, \varphi_{m}\rangle = 0$ for any $i$ where $p_{i}$ is not contained in the same cell as $p_{m}$. More on this in \ref{Sparsity}

In almost all applications, we use that 
$\langle \mathcal{L}\varphi_{i}, \varphi_{j}\rangle = \int_{\Omega}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. This makes assembly of the global system much simpler because we can write the integral as a sum over the cells $ \int_{\Omega}\mathcal{L}\varphi_{i}, \varphi_{j}dx = \sum_{\tau} \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. With this we now have what is called the cell (local) matrix 
$$ \mathbf{A}^{cell}_{i,j} = \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx\quad i, j = 1, \ldots k $$ 
again $k$ is the number of degrees of freedom per cell. In computing these local matrices, we then distribute them to the global matrix $ \mathbf{A} = \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} $. Where $P_{tau}$ accounts for the mapping of global indices to local indices. Now the matrix assembly process is done cellwise to compute the cell matrix, then distributed to the global matrix. 

One aspect that can make the local assembly process simpler is to assign the dofs to a reference cell and then use a change of coordinates to write the cell integral as an integral over the reference cell. This simply requires that we use the jacobian of the transformation to transform operations from the cell to the reference cell.

Once the matrix is assembled the linear system is then typically solved using an iterative method such as GMRES or Conjugate Gradient method. These methods are prefered due to only requiring matrix multiplications since when $\mathbf{A}$ is sparse, then the the complexity of a matrix-vector multiplication is only $O(n)$. 

This description of the finite element method has highlighted key features, cell wise matrix assembly, and iterative linear solvers. As will be discussed in \ref{later}, these are the two key bottle necks that limit performance in most finite element codes which this project aims to address. 

\KW{Add citations}


\subsection{Sparse matrix-vector multiplication}

\KW{Discuss number of dofs per row, CSR format, bottleneck in assembly and bottleneck in Vmult. Stick to sparse matix only, find citation?} 
\AZ{Talk about sparsity of the system and typical operation required (MV) for iterative solvers, emphasize problems related to memory bandwidth}


One usually builds the basis~\eqref{basis} using a concept of \textit{finite element}, see~\cite{ciarlet2002finite}. Roughly speaking, the finite element is a collection of 3 entities: cell (typically a convex polygon in 2D or polyhedron 3D), finite dimensional space of shape functions~$S$ defined on this cell (typically polynomials of some degree~$k$), and the set of degrees of freedom for these shape functions (e.g. nodal values, mean values over edges or faces, normal or tangential components etc.) Given finite element, one then constructs a basis in~$S$ via associating a shape function with a degree of freedom: One fixes one degree of freedom to be unity and all the others to be zero for this shape function. 

When the finite element is chosen, one constructs a trace (on a given cell) of the basis function~${\phi \in \vect\phi}$ from cell's shape function. This way it is guaranteed that each basis function has a compact support. See Figure~\AZ{Add.}

In this report we stick to \textit{Lagrange} (this means that degrees of freedom are chosen to be nodal values) finite elements of polynomial degree~$k > 1$ defined on hexahedron elements. \AZ{Add quad elements too?} As we will see below, theoretically the matrix-free approach becomes more and more beneficial as one increases~$k$, i.e. for higher-order elements. 

A matrix $\vect B \in \mathbb R^{\dimSize\times\dimSize}$ is called \textit{sparse} iff $\sum_{i,\,j} \mbox{sign}\,b_{ij} = O(\dimSize)$ as $\dimSize \rightarrow \infty$, i.e. most of the elements are zero and need not to be stored. This way memory requirements are $O(\dimSize)$ vs. $\dimSize^2$ as in dense case. \textbf{Given some requirements on the mesh and the finite element definition, we have that our system matrix~$\vect A$ in~\eqref{system} is sparse.} \AZ{Add an example for P1 elements.}

For big complex problems, especially in 3D, it is typical to use an iterative solver to solve~\eqref{system}. The problem may combine different variables (scalar and vector) and couple different physics (e.g. electromagnetics and fluid flow, fluid-structure interaction and so forth); In this case one would typically equip the iterative solver with an appropriate preconditioner (which may be rather complex) that takes the structure of the underline problem into account. In any case, both the solver and the application of the preconditioner are usually based on one core operation: \textbf{Matrix-vector multiplication}.

One uses special matrix formats to represent sparse matrices in a computer to achieve~$O(\dimSize)$ memory requirements. One popular choice is \textit{compressed sparse row} (CSR) storage format, see~\AZ{Add ref and describe the format.} This format is quite economic in terms of memory requirements and is convenient to perform the matrix-vector multiplication:

\AZ{Insert algorithm. Point out the drawbacks with cache.}

\subsection{Matrix-free approach to matrix-vector multiplication}

In consideration of the bottlenecks a drawbacks presented in \ref{above}, there has been much research in how to avoid them; one such approach is known as the Matrix-Free finite element method. This method has technically been known for a long time \cite{first paper}, it has only recently become popular with the advent of GPGPU computing\cite{recent papers}. 

The idea of the matrix-free fem is to avoid memory-related bottlenecks by avoiding computation fo the global matrix all together. And taking advantage of the cell-based assembly structe of the system. 

Basic implementation
We begin with the cell based struction discued in \ref{}. $ \mathbf{A} = \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} $. We note that each row and column in the global matrix correspond to a single dof. Now we compute the matrix-vector product
\begin{align*}
\mathbf{A}x &= \left(\sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} \right)x \\
				&= \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau}x \\
				&= \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}x_{cell}.
\end{align*}
Here $x_{cell} \in \mathbb{R}^{k}$ is a vector of values at the degrees of freedom from the given cell. This being know, we do not need to compute $P$ in practice, only keep track of the local to global numbering of the dofs. Now we compute $y_{cell} = \mathbf{A}^{cell}x_{cell}$ which then alows us to compute
\begin{align*}
\mathbf{A}x &= \sum_{\tau}P^{T}_{\tau}y_{cell} \\
				&= \sum_{\tau} y_{\tau}
\end{align*}
Now $y_{tau} \in \mathbb{R}^{N}$ is the vector with entries corresponding to contributions to the result from the given cell $\tau$. In practice, this can be done in one of two ways. One approach is to have distribute cells to various processors and have the processors compute the contribution from the locally owned cells and then to do a distributed reduction to gather the result into a single vector; this is method is what is typically used when using a distributed parallelization model such as MPI. The other approach is to assign each cell to a process and have the process compute the contributions from that cell and place the result directly into the resultant vector since the index mapping is known; this approach is more suitable for a shared memory approch such as OpenMP or Cuda OpenACC. It is also possible to combine the two approaches when using a MPI + X parallelization model. In this project we use the approach of assigning each cell to a thread using OpenMP, more in \ref{implementation }. For the remainder of this report we use be discussing the method using the OpenMp approach unless otherwise specified.

It is important to note that there are no approximations being made and that the result of a standard matrix-vector multiplication and applicaiton of this matrix-free approach will give the exact same result. The only difference is that for the matrix-free approach we compute the cell matrices ``on the fly'' as needed; as opposed to precomputing them and storing them in a global matrix. This will obviously require a signifcantly larger number of flops, however the goal is to have a method that is cpu bounded and scales well as the number of processes is scaled up. The matrix free method ideally avoids memory speed limitations by not needing to access a global matrix at all. These benefits should become more visible as the matrix size become large and as the number of degrees of freedom per cell increases as well. For 2D problems with only a few degrees of freedom per cell the method is not expected to be as competative due to the overhead associated with each cell. 

With the consideration that there will be a large overhead in computing the local matrix-vector product, there are a few optimizations that can be made. If for example we have that 
$$\mathbf{A}^{cell}_{i,j} = \int_{\tau}\nabla \alpha(x) \varphi_{i}(x) \cdot \nabla \varphi_{j}(x) dx\quad i,j = 1 \ldots k$$ 
then we can do a change of coordinates to a reference cell
$$\mathbf{A}^{cell}_{i,j} = \int_{ref} \alpha(T(\hat{x}) (J^{-T}\nabla \varphi_{i}(\hat{x})) \cdot (J^{-T}\nabla \varphi_{j}(\hat{x}))\,\vert \mathrm{det}J(T\hat{x}) \vert d\hat{x} \quad i,j = 1 \ldots k$$ 
If this can then be rewritten as 
\KW{make bold}
$$\mathbf{A}^{cell}_{i,j} = B^{T}_{ref}J^{-1}_{cell}D_{cell}J^{-T}_{cell}B_{ref}$$
Where $B_{ref}$ is a matrix of values of the gradient on the referece cell at each quadrature point, $J$ is the jacobian of the transformation from the reference cell to the cell, and $D$ is a diagonal matrix with values of $\alpha(T\hat{x})\, \vert \mathrm{det}J(T\hat{x}) \vert$ along the diagonal. This is bennificial because the  values of $B^{T}$ are the same for each cell and can be pre-computed.

Note that if we were building a global matrix, this matrix product would need to be computed in order to distribute it to the global matrix. This grows more time consuming as the number dofs per cell grows, or as the nmber of integration points grows (typically the nubmer of quadrature points is approximatly equal to the degrees of freedom per cell). However, if we simply need to evaluate $A^{cell}x_{cell}$, we can use the decomposition to compute a series of matrix-vector products instead of having to multiply all of these matrices together. This reduces has the benefit of reducing the complexity of the work done per cell. 





Optimizations reference cell 

Benefits Complexity 

\begin{enumerate}
% \item Matrix-free methods skip the storage of big global sparse matrices and compute the underlying weak forms on the fly. Since the memory transfer, i.e., the speed at which the data can be read from RAM memory, is the bottleneck for matrix-based computations rather than the actual arithmetic done using this data, a matrix-free evaluation that reads less data can be advantageous even if it does more computations. This concept is building upon a trend in computer architecture which is best described by the term memory wall, saying that compute performance has increased more rapidly than the memory performance. Thus, a certain degree of arithmetic operations is essentially for free, and this share has become larger during the last twenty years. It has enabled this radical algorithm switch going from a matrix-based to a matrix-free implementation of matrix-vector products for iterative solvers, besides their classical use in explicit time integration. Of course, the implementation must be efficient and there cannot be an excess in computations to make it a win in total. The deal.II library uses SIMD vectorization and highly optimized kernels based on templates of the polynomial degree to achieve this goal. To give a perspective, a sparse matrix-vector product for quadratic elements FE\_Q used to be equally fast as the matrix-free implementation on processors designed around 2005-2007 (e.g. Pentium 4 or AMD Opteron Barcelona with 2-4 cores per chip). By 2018, the matrix-free evaluation is around eight times as fast (measured on Intel Skylake Server, 14 cores).
\item Matrix-free methods have a better complexity per degree of freedom as the degree is increased, due to sum factorization. The work per degree of freedom increases as (k) in the degree k for matrix-free schemes, whereas it increases as (kd) for matrix-based methods. This gives higher order schemes an edge. A particularly nice feature in matrix-free evaluation is that the (1) terms often dominate, so it appears that higher order methods are as fast in terms of evaluation time as low order ones, when they have the same number of degrees of freedom. For the implementation in deal.II, best throughput is typically achieved for polynomial degrees between three and six.
\end{enumerate}

\section{Implementation details}

\AZ{Refer to deal.II step; Point out what was changed and why; Explain compilation details}

\subsection{Parallelization Details}

\section{Computational experiments}

\section{Results}

\subsection{Analysis of results}

\subsection{Random Issues we ran in to}

\subsection{Summary}


\bibliographystyle{plain}
\bibliography{bibl}

\end{document}