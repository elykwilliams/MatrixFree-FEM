\documentclass[12pt]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[dvipsnames, table]{xcolor}
\colorlet{DarkRed}{Red!90!black}
\colorlet{LightRed}{Red!10!white}
\colorlet{DarkGreen}{Green!50!black}
\colorlet{LightGreen}{Green!10!white}
\usepackage{colortbl} % https://texblog.org/2011/04/19/highlight-table-rowscolumns-with-color/

% sections
\usepackage{titlesec} % https://latex.org/forum/viewtopic.php?t=10456
\titleformat*{\section}{\large\bfseries}
\titlespacing*{\section}{0pt}{2mm}{2mm}
\titleformat{\subsection}[runin]% runin puts it in the same paragraph
{\normalfont\bfseries}% formatting commands to apply to the whole heading
{\thesubsection}% the label and number
{0.5em}% space between label/number and subsection title
{}% formatting commands applied just to subsection title
[.]% punctuation or other commands following subsection title
%\titleformat{\section}[runin]{\normalfont\bfseries}{\thesection}{0.5em}{}[.]
\titleformat{\subsubsection}[runin]{\normalfont\bfseries}{\thesubsubsection}{0.5em}{}[.]

% links
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	linkcolor={DarkRed},
	citecolor={DarkRed},
	urlcolor={blue}
}

\usepackage{geometry}
\newgeometry{
	left=2cm, right=1.5cm, top=1.5cm, bottom=1.5cm,
	includefoot, heightrounded
}

\usepackage[parfill]{parskip} % https://tex.stackexchange.com/a/16703/135296

% sub figures / grids of pictures
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{{img/}} % includegraphics path
% \usepackage[export]{adjustbox} % https://tex.stackexchange.com/questions/20640/how-to-add-border-for-an-image
\newcommand{\includegraphicsw}[2][1.]{\includegraphics[width=#1\linewidth]{#2}}
\newcommand{\svginput}[1]{\input{img/#1}} % pdf_tex path
\newcommand{\svginputw}[2][\linewidth]{\def\svgwidth{#1}\input{img/#2}} % pdf_tex path

% tables
\usepackage{longtable}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{float} % for H

% bold for everything
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% differentials
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Dist}{dist}
\newcommand{\sphere}{{\Gamma_{\text{sph}}}}
\newcommand{\tor}{{\Gamma_{\text{tor}}}}

\newcommand{\HOne}{{\mathbb H^1}}
\newcommand{\LTwo}{{\mathbb L^2}}
\newcommand{\LTwoSpace}[1][\Gamma]{{\mathbb L^2\left({#1}\right)}}
\newcommand{\HOneSpace}[1][\Gamma]{{\mathbb H^1\left({#1}\right)}}

\newcommand{\cl}[1]{\texttt{\$~#1}}

\usepackage{listings}
%\definecolor{mygreen}{rgb}{0,0.6,0}
\lstset{
%	language=C++,
%	basicstyle=\footnotesize\ttfamily,
	breaklines=true,
%	commentstyle=\color{mygreen},
	frame=l,
	xleftmargin=5pt,
	tabsize=2,
%	belowskip=-1pt
} 

% https://tex.stackexchange.com/questions/9425/how-to-fix-footnote-position-at-the-bottom-of-the-page
\usepackage[bottom]{footmisc}
\newcommand{\AZ}[1]{{\color{red}\textbf{AZ}:~#1}}
\newcommand{\KW}[1]{{\color{blue}\textbf{KW}:~#1}}
\newcommand{\dimSize}{N}
\newcommand{\dofspercell}{k}

\title{COSC\,6365 -- Introduction to High Performance Computing, Fall 2019\\Final project: \textbf{Matrix-free finite element method}}
\author{
	Kyle Williams\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (kylew@math.uh.edu).} \and
	Alexander Zhiliakov\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (alex@math.uh.edu).}
}

\begin{document}
	
\maketitle

\tableofcontents
\vfill
\clearpage
\let\oldtabular\tabular
\renewcommand{\tabular}[1][1.5]{\def\arraystretch{#1}\oldtabular}
\renewcommand\arraystretch{1.3}

\section{Theoretical background}

\subsection{Finite element method}\label{sec:fem}
The finite element method is a general method for solving partial differential equations~\cite{smth}. So we begin with an abstract partial differential equation
\begin{equation}\label{pde}
\begin{cases} 
	\text{Find } u \in V(\Omega) \text{ such that } \\
	\langle \mathcal{L}u, v \rangle = \langle f, v \rangle \; \forall v \in V(\Omega)
\end{cases}
\end{equation}
written in a weak form. Here~$\Omega$ is the domain on which we wish to solve the differential equation~\eqref{pde}, and $V \equiv V(\Omega)$ is an appropriate vector space depending on the particular differential equation and boundary conditions in question. 

Generally this vector space~$V$ will be infinite dimensional and will require some sort of approximation if we wish to solve the problem numerically. In the finite element method, we use the Ritz-Galerkin approach to approximation: Instead of choosing an infinite dimensional vector 
space, we choose a finite dimensional subspace~$V_{h}$, so the problem becomes
\begin{equation}\label{finite}
\begin{cases} 
	\text{Find } u_{h} \in V_{h} \text{ such that } \\
	\langle \mathcal{L}u_{h}, v_{h} \rangle = \langle f, v_{h} \rangle \; \forall v_{h} \in V_{h}.
\end{cases}
\end{equation}
Since $V_{h}$ is a finite dimensional, we can choose some basis functions~$\vect\phi \coloneqq \{\varphi_1, \varphi_2, \dots, \varphi_\dimSize\}$ spanning~$V_h$, and with this we can represent $u_{h} = \sum_{i = 1}^{\dimSize} U_{i}\varphi_{i}$. Note that we only need to test that the equality~\eqref{finite} holds for each basis function, i.e.~\eqref{finite} is equivalent to
\begin{equation}\label{fem}
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{\dimSize} \text{ such that } \\
	\sum_{i = 1}^{\dimSize}\langle \mathcal{L}U_{i}\varphi_{i}, \varphi_{j} \rangle = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots,\dimSize.
\end{cases}
\end{equation}
If the differential equation~\eqref{pde} is linear, then~\eqref{fem} becomes a linear system
\[
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{N} \text{ such that } \\
	\sum_{i = 1}^{N}\langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle U_{i} = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots,\dimSize,
\end{cases}
\]
or
\begin{equation}\label{system}
	\vect A\,\vect x = \vect b
\end{equation}
with $\mathbf{A}_{i,j} = \langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle,\; \vect x_{i} = U_{i}$, and $\mathbf{b}_{i} = \langle f, \varphi_{i} \rangle$ for the right hand side. The \textit{finite element method} is a method for choosing these basis functions in such a way that the resulting system will have certain specified properties, e.g. one may require~$\mathbf{A}$ to be sparse, reasonably well-conditioned etc.

In the finite element method to choose the basis~$\vect\phi$, we first take the domain $\Omega$ and build its discrete representation~$\Omega_h$: We triangulate~$\Omega$ into a collection of polyhedral cells (or elements), typically tetrahedrons of hexahedrons. See Figure~\ref{fig:mesh}.

%One usually builds the basis~$\vect\phi$ using a concept of \textit{finite element}, see~\cite{ciarlet2002finite}. Roughly speaking, the finite element is a collection of 3 entities: Cell (typically a convex polygon in 2D or polyhedron 3D), finite dimensional space of shape functions~$S$ defined on this cell (typically polynomials of some degree~$k$), and the set of degrees of freedom (d.o.f.) for these shape functions (e.g. nodal values, mean values over edges or faces, normal or tangential components etc.) Given finite element, one then constructs a basis in~$S$ via associating a shape function with a d.o.f.: One fixes one degree of freedom to be unity and all the other d.o.f. to be zero for this shape function. 
%
%When the finite element is chosen, one constructs a trace (on a given cell) of the basis function~${\phi \in \vect\phi}$ from cell's shape function. This way it is guaranteed that each basis function has a compact support. See Figure~\AZ{Add.}

On each cell of~$\Omega_h$ we assign a set of points $p$ called degrees of freedom (d.o.f.); The basis functions are then chosen so that $\varphi_{i}(p_{j}) = \delta_{i}^{j}$, i.e. $i$th basis function is unity on its associated d.o.f. and is zero on all the other d.o.f. %This will have the effect of each basis function being a polynomial of degree $k$ with $k$ being the number of d.o.f. per cell.

\begin{figure}[H]
	\centering
	\includegraphicsw[.4]{mesh.png}
	\caption{Triangulation~$\Omega_h$ of the unit cube domain~$\Omega = (0, 1)^{3}$ consisting of hexahedral cells}\label{fig:mesh}
\end{figure}

With this particular choice, for cells that do not contain, say, the d.o.f. $p_{m}$, the corresponding basis function $\varphi_{m}$ will also be zero. This means that $\langle \mathcal{L}\varphi_{i}, \varphi_{m}\rangle = 0$ for any $i$ where $p_{i}$ is not contained in the same cell as $p_{m}$. More on this in \ref{Sparsity}

In this report we stick to \textit{Lagrange} (this means that d.o.f. are chosen to be nodal values) finite elements of polynomial degree~$K > 1$ defined on hexahedron elements, i.e. basis function $\phi_i \in \vect\phi$ is a piecewise polynomial that is nonzero only in cells sharing vertex~$i$ of~$\Omega_h$. As we will see in Section~\ref{sec:mfree}, theoretically the matrix-free approach becomes more and more beneficial as one increases~$K$, i.e. for higher-order elements. 

In almost all applications, we use that 
$\langle \mathcal{L}\varphi_{i}, \varphi_{j}\rangle = \int_{\Omega}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. This makes assembly of the global system~\eqref{system} much simpler because we can write the integral as a sum over the cells $ \int_{\Omega}\mathcal{L}\varphi_{i}, \varphi_{j}dx = \sum_{\tau\in\Omega_h} \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. With this we now have what is called the cell (local) matrix 
$$ 
	\mathbf{A}^{cell}_{i,j} = \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx\quad i, j = 1, \ldots k,
$$ 
with $k$ being the number of degrees of freedom per cell. In computing these local matrices, we then distribute them to the global matrix $ \mathbf{A} = \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} $. Where $P_{tau}$ accounts for the mapping of global indices to local indices. The matrix assembly process is done cell-wise to compute the cell matrices that are then distributed to the global matrix~$\vect A$. 

One aspect that can make the local assembly process simpler is to assign the dofs to a reference cell and then use a change of coordinates to write the cell integral as an integral over the reference cell. This simply requires that we use the jacobian of the transformation to transform operations from the cell to the reference cell.

Once the matrix is assembled the linear system is then typically solved using an iterative method such as GMRES or Conjugate Gradient method. These methods are prefered due to only requiring matrix multiplications since when $\mathbf{A}$ is sparse, then the the complexity of a matrix-vector multiplication is only $O(n)$. 

This description of the finite element method has highlighted key features, cell wise matrix assembly, and iterative linear solvers. As will be discussed in \ref{later}, these are the two key bottle necks that limit performance in most finite element codes which this project aims to address. 

\KW{Add citations}

\subsection{Sparse matrix-vector multiplication}

A matrix $\vect B \in \mathbb R^{\dimSize\times\dimSize}$ is called \textit{sparse} iff $\sum_{i,\,j} \mbox{sign}\,\vect B_{i,j} = O(\dimSize)$ as $\dimSize \rightarrow \infty$, i.e. most of the elements are zero and need not to be stored. This way memory requirements are $O(\dimSize)$ vs. $\dimSize^2$ as in dense case. Given some requirements on the mesh~$\Omega_h$ and the choice of~$\vect\phi$ as explained in Section~\ref{sec:fem}, we have that our system matrix~$\vect A$ in~\eqref{system} is sparse, see Figure~\ref{fig:sparse}.

For big complex problems, especially in 3D, it is typical to use an iterative solver to solve~\eqref{system}. The problem may combine different variables (scalar and vector) and couple different physics (e.g. electromagnetics and fluid flow, fluid-structure interaction and so forth); In this case one would typically equip the iterative solver with an appropriate preconditioner (which may be rather complex) that takes the structure of the underline problem into account. In any case, both the solver and the application of the preconditioner are usually based on one core operation: \textit{Matrix-vector multiplication}
\begin{equation}\label{mv}
	\vect x \mapsto \vect A\,\vect x \eqqcolon \vect z.
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphicsw[.4]{sparse.pdf}
	\caption{Sparsity pattern of~$\vect A$}\label{fig:sparse}
\end{figure}

One uses special matrix formats to represent sparse matrices in a computer to achieve~$O(\dimSize)$ memory requirements. One popular choice is \textit{compressed sparse row} (CSR) storage format, see e.g.~\cite{saad2003iterative}. This format is quite economic in terms of memory requirements and is convenient to perform the matrix-vector multiplication:
\begin{lstlisting}[basicstyle=\small,caption={Implementation of the sparse matrix-vector multiplication~\eqref{mv} for CSR format},label={lst:csrmv},captionpos=b]
	for (i = 0; i < nrows; ++i) 
		for (k = rowptr[i]; k < rowptr[i+1]; ++k)
			z[i] += val[k] * x[colind[k]];
\end{lstlisting}
Here the sparse matrix~$\vect A$ is represented with 3 vectors:
\begin{enumerate}
	\item Vector of values~\texttt{val} of size~$O(n)$ is the nonzero values of~$\vect A$ in row-by-row fashion,
	\item Vector of column indexes~\texttt{colind} with the same size as~\texttt{val} and with \texttt{colind[i]} being a column index of the element~\texttt{val[i]}, and 
	\item Vector of row pointers~\texttt{rowptr} of size $\dimSize + 1$ is a vector of indexes s.t. \texttt{rowptr[i]} is the starting index for the $i$th row of~$\vect A$, i.e. \texttt{val[rowptr[i]]} is the first nonzero element in the $i$th row and \texttt{rowptr[i + 1] - rowptr[i]} is the number of nonzero elements of $i$th row.
\end{enumerate}

CSR is optimal in the sense that both storage and multiplication require~$O(n)$ resources. However, one may note two major issues of the CSR multiplication implementation in Listing~\ref{lst:csrmv}:
\begin{enumerate}
	\item there is no spatial locality for~$\vect x$, and
	\item there are 5 memory refs vs. 2 arithmetic operations, i.e. the routine is memory bandwidth bounded.
\end{enumerate}
Moreover, the assembly of~$\vect A$ itself is a problem, for it requires explicit computation of the matrix elements (or elements of cell matrices). The alternative approach for implementing~\eqref{mv} that tries to deal with these issues is described in the next section.

\subsection{Matrix-free approach to matrix-vector multiplication}\label{sec:mfree}

In consideration of the bottlenecks a drawbacks presented in \ref{above}, there has been much research in how to avoid them; one such approach is known as the Matrix-Free finite element method. This method has technically been known for a long time \cite{first paper}, it has only recently become popular with the advent of GPGPU computing\cite{recent papers}. 

The idea of the matrix-free fem is to avoid memory-related bottlenecks by avoiding computation fo the global matrix all together. And taking advantage of the cell-based assembly structe of the system. 

Basic implementation
We begin with the cell based struction discued in \ref{}. $ \mathbf{A} = \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} $. We note that each row and column in the global matrix correspond to a single dof. Now we compute the matrix-vector product
\begin{align*}
\mathbf{A}x &= \left(\sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau} \right)x \\
				&= \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}P_{\tau}x \\
				&= \sum_{\tau}P^{T}_{\tau}\mathbf{A}^{cell}x_{cell}.
\end{align*}
Here $x_{cell} \in \mathbb{R}^{k}$ is a vector of values at the degrees of freedom from the given cell. This being know, we do not need to compute $P$ in practice, only keep track of the local to global numbering of the dofs. Now we compute $y_{cell} = \mathbf{A}^{cell}x_{cell}$ which then alows us to compute
\begin{align*}
\mathbf{A}x &= \sum_{\tau}P^{T}_{\tau}y_{cell} \\
				&= \sum_{\tau} y_{\tau}
\end{align*}
Now $y_{tau} \in \mathbb{R}^{N}$ is the vector with entries corresponding to contributions to the result from the given cell $\tau$. In practice, this can be done in one of two ways. One approach is to have distribute cells to various processors and have the processors compute the contribution from the locally owned cells and then to do a distributed reduction to gather the result into a single vector; this is method is what is typically used when using a distributed parallelization model such as MPI. The other approach is to assign each cell to a process and have the process compute the contributions from that cell and place the result directly into the resultant vector since the index mapping is known; this approach is more suitable for a shared memory approch such as OpenMP or Cuda OpenACC. It is also possible to combine the two approaches when using a MPI + X parallelization model. In this project we use the approach of assigning each cell to a thread using OpenMP, more in \ref{implementation }. For the remainder of this report we use be discussing the method using the OpenMp approach unless otherwise specified.

It is important to note that there are no approximations being made and that the result of a standard matrix-vector multiplication and applicaiton of this matrix-free approach will give the exact same result. The only difference is that for the matrix-free approach we compute the cell matrices ``on the fly'' as needed; as opposed to precomputing them and storing them in a global matrix. This will obviously require a signifcantly larger number of flops, however the goal is to have a method that is cpu bounded and scales well as the number of processes is scaled up. The matrix free method ideally avoids memory speed limitations by not needing to access a global matrix at all. These benefits should become more visible as the matrix size become large and as the number of degrees of freedom per cell increases as well. For 2D problems with only a few degrees of freedom per cell the method is not expected to be as competative due to the overhead associated with each cell. 

With the consideration that there will be a large overhead in computing the local matrix-vector product, there are a few optimizations that can be made. If for example we have that 
$$\mathbf{A}^{cell}_{i,j} = \int_{\tau}\nabla \alpha(x) \varphi_{i}(x) \cdot \nabla \varphi_{j}(x) dx\quad i,j = 1 \ldots \dofspercell$$ 
then we can do a change of coordinates to a reference cell
$$\mathbf{A}^{cell}_{i,j} = \int_{ref} \alpha(T(\hat{x}) (J^{-T}\nabla \varphi_{i}(\hat{x})) \cdot (J^{-T}\nabla \varphi_{j}(\hat{x}))\,\vert \mathrm{det}J(T\hat{x}) \vert d\hat{x} \quad i,j = 1 \ldots \dofspercell$$ 
If this can then be rewritten as 
\KW{make bold}
$$\mathbf{A}^{cell}_{i,j} = B^{T}_{ref}J^{-1}_{cell}D_{cell}J^{-T}_{cell}B_{ref}$$
Where $B_{ref}$ is a matrix of values of the gradient on the referece cell at each quadrature point, $J$ is the jacobian of the transformation from the reference cell to the cell, and $D$ is a diagonal matrix with values of $\alpha(T\hat{x})\, \vert \mathrm{det}J(T\hat{x}) \vert$ along the diagonal. This is bennificial because the  values of $B^{T}$ are the same for each cell and can be pre-computed.

Note that if we were building a global matrix, this matrix product would need to be computed in order to distribute it to the global matrix. This grows more time consuming as the number dofs per cell grows, or as the nmber of integration points grows (typically the nubmer of quadrature points is dependent on the number of $\dofspercell$. However, if we simply need to evaluate $A^{cell}x_{cell}$, we can use the decomposition to compute a series of matrix-vector products instead of having to multiply all of these matrices together. This reduces has the benefit of reducing the complexity of the work done per cell. This has the effect of reducting the complexity per cell from $O(\dofspercell^{dim})$ to $O(\dofspercell)$; so we can expect that the matrix-free approach will be more competative as the number of $\dofspercell$ grows.


\section{Implementation details}
For this project we used the deal.II finite element library to handle the domain triangulation, dof enumeration, and the local to global index mapping. The two matrix multiplication methods were both implemented manually. The deal.II library has a tutorial on how to use their builtin matrix-free method; as well as a description of how the method is implemented internally. We used this description with modifications to implement our methods. In particular, the deal.II matrix-free builtin methods are designed around an MPI based parallelization \ref{above}; whereas we intended to use OpenMP based parallelization. We tested our implementation against deal.II's builit matrix-vector multiplication to ensure that they were in fact the same.
\subsection{Sparse matrix}

\subsection{Matrix-free}
In this section we discuss the matrix-free vmult function that will take an input vector and compute the product $Ax$. In our implementation, this is a class member so a number of variables relating to the referece cell have been pre-computing in construction of the class.

Using the description above, we being with a loop over each cell. On each cell, we generate an omp task to compute the contributions from each cell.
\begin{lstlisting}
#pragma omp parallel
#pragma omp single
{
  	for(cell = dofh.begin_active(); cell != dofh.end(); cell++){
		#pragma omp task firstprivate(cell)
		{
			// cell computation
		}
	}
	#pragma omp taskwait
}            
\end{lstlisting}

We decided to use omp task because we are only able to iterate over deal.II's iterator type. Additionally, when the body of the loop is large, it can be beneficial to use an omp task as opposed to an omp parallel for. 

Then for each cell we do the following
\begin{enumerate}
\item Initialize quadrature points and weights, the inverse of the jacobian and its determinant
\item Initialize the local to global mapping
\item Precompute $\alpha(q)$ at each quadrature point
\item Extract input vector values at local dofs
\begin{lstlisting}
	Vector<double> cell_vec(dofs_per_cell);
	for (unsigned int i = 0; i<dofs_per_cell; ++i)
			cell_vec(i) = src(local_to_global[i]);
\end{lstlisting}
\item Apply $B_{ref}$ to 
\item Apply $J^{-T}$
\item Apply D
\item Apply $J^{-1}$
\item Apply $B^{T}_{ref}$
\item Distribute local result to global result vector
\begin{lstlisting}
for(unsigned int i=0; i<dofs_per_cell; i++){
#pragma omp atomic
	dst(local_to_global[i]) += cell_vec_dest(i);
}
\end{lstlisting}
\end{enumerate}
We have to use atomic to avoid race conditions in accessing the shared global result. This could be avoided in two ways, one is to apply a coloring scheme to each cell so that no task accesses the same dof at the same time, or each thread could have a private global vector and then would to a reduction to merge them all together. We did not choose the later approach since it would create a very large memory footprint per thread. And in the former case, was outside the scope of our project.


\section{Computational experiments}

\section{Results}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{clTime.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{mfTime.pdf}
	\end{subfigure}%
	\caption{Excecution time: CSR (left) and matrix-free (right) approaches}\label{fig:time}
\end{figure}

\begin{table}[H]
	\centering\caption{Best thread team sizes (see Figure~\ref{fig:time})}
	\includegraphicsw[.9]{bestTime.pdf}
\end{table}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{cache.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{inst.pdf}
	\end{subfigure}
	\caption{Cache miss ratio (left) and instructions per cycle (right) for the largest matrix size~$n = 912\,673$}
\end{figure}

\begin{table}[H]
	\centering\caption{Energy consumption for a single thread; TDP = 150\,W}
	\includegraphicsw[.9]{rapl.pdf}
\end{table}

\subsection{Analysis of results}

\subsection{Random Issues we ran in to}

\subsection{Summary}


\bibliographystyle{plain}
\bibliography{bibl}

\end{document}