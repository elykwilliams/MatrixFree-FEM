\documentclass[12pt]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[dvipsnames, table]{xcolor}
\colorlet{DarkRed}{Red!90!black}
\colorlet{LightRed}{Red!10!white}
\colorlet{DarkGreen}{Green!50!black}
\colorlet{LightGreen}{Green!10!white}
\usepackage{colortbl} % https://texblog.org/2011/04/19/highlight-table-rowscolumns-with-color/

% sections
\usepackage{titlesec} % https://latex.org/forum/viewtopic.php?t=10456
\titleformat*{\section}{\large\bfseries}
\titlespacing*{\section}{0pt}{2mm}{2mm}
\titleformat{\subsection}[runin]% runin puts it in the same paragraph
{\normalfont\bfseries}% formatting commands to apply to the whole heading
{\thesubsection}% the label and number
{0.5em}% space between label/number and subsection title
{}% formatting commands applied just to subsection title
[.]% punctuation or other commands following subsection title
%\titleformat{\section}[runin]{\normalfont\bfseries}{\thesection}{0.5em}{}[.]
\titleformat{\subsubsection}[runin]{\normalfont\bfseries}{\thesubsubsection}{0.5em}{}[.]

% links
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	linkcolor={DarkRed},
	citecolor={DarkRed},
	urlcolor={blue}
}

\usepackage{geometry}
\newgeometry{
	left=2cm, right=1.5cm, top=1.5cm, bottom=1.5cm,
	includefoot, heightrounded
}

\usepackage[parfill]{parskip} % https://tex.stackexchange.com/a/16703/135296

% sub figures / grids of pictures
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{{img/}} % includegraphics path
% \usepackage[export]{adjustbox} % https://tex.stackexchange.com/questions/20640/how-to-add-border-for-an-image
\newcommand{\includegraphicsw}[2][1.]{\includegraphics[width=#1\linewidth]{#2}}
\newcommand{\svginput}[1]{\input{img/#1}} % pdf_tex path
\newcommand{\svginputw}[2][\linewidth]{\def\svgwidth{#1}\input{img/#2}} % pdf_tex path

% tables
\usepackage{longtable}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{float} % for H

% bold for everything
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% differentials
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Dist}{dist}
\newcommand{\sphere}{{\Gamma_{\text{sph}}}}
\newcommand{\tor}{{\Gamma_{\text{tor}}}}

\newcommand{\HOne}{{\mathbb H^1}}
\newcommand{\LTwo}{{\mathbb L^2}}
\newcommand{\LTwoSpace}[1][\Gamma]{{\mathbb L^2\left({#1}\right)}}
\newcommand{\HOneSpace}[1][\Gamma]{{\mathbb H^1\left({#1}\right)}}

\newcommand{\cl}[1]{\texttt{\$~#1}}

\usepackage{listings}
\definecolor{mygreen}{rgb}{0,0.6,0}
\lstset{
	language=C++,
	basicstyle=\small\ttfamily,
	breaklines=true,
	commentstyle=\color{mygreen},
	frame=l,
	xleftmargin=5pt,
	tabsize=2,
	belowskip=-1pt
} 

% https://tex.stackexchange.com/questions/9425/how-to-fix-footnote-position-at-the-bottom-of-the-page
\usepackage[bottom]{footmisc}
\newcommand{\AZ}[1]{{\color{red}\textbf{AZ}:~#1}}
\newcommand{\KW}[1]{{\color{blue}\textbf{KW}:~#1}}
\newcommand{\dimSize}{N}
\newcommand{\dofspercell}{k}

\title{COSC\,6365 -- Introduction to High Performance Computing, Fall 2019\\Final project: \textbf{Matrix-free finite element method}}
\author{
	Kyle Williams\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (kylew@math.uh.edu).} \and
	Alexander Zhiliakov\thanks{Department of Mathematics, University of Houston, Houston, Texas 77204 (alex@math.uh.edu).}
}

\usepackage[nottoc,notlot,notlof]{tocbibind}

\begin{document}
	
\maketitle

\tableofcontents
\vfill
\clearpage
\let\oldtabular\tabular
\renewcommand{\tabular}[1][1.5]{\def\arraystretch{#1}\oldtabular}
\renewcommand\arraystretch{1.3}

\section{Theoretical background}

\subsection{Finite element method}\label{sec:fem}
The finite element method is a general method for solving partial differential equations~\cite{ciarlet2002finite}. So we begin with an abstract partial differential equation
\begin{equation}\label{pde}
\begin{cases} 
	\text{Find } u \in V(\Omega) \text{ such that } \\
	\langle \mathcal{L}u, v \rangle = \langle f, v \rangle \; \forall v \in V(\Omega)
\end{cases}
\end{equation}
written in a weak form. Here~$\Omega$ is the domain on which we wish to solve the differential equation~\eqref{pde}, and $V \equiv V(\Omega)$ is an appropriate vector space depending on the particular differential equation and boundary conditions in question. 

Generally this vector space~$V$ will be infinite dimensional and will require some sort of approximation if we wish to solve the problem numerically. In the finite element method, we use the Ritz-Galerkin approach to approximation: Instead of choosing an infinite dimensional vector 
space, we choose a finite dimensional subspace~$V_{h}$, so the problem becomes
\begin{equation}\label{finite}
\begin{cases} 
	\text{Find } u_{h} \in V_{h} \text{ such that } \\
	\langle \mathcal{L}u_{h}, v_{h} \rangle = \langle f, v_{h} \rangle \; \forall v_{h} \in V_{h}.
\end{cases}
\end{equation}
Since $V_{h}$ is a finite dimensional, we can choose some basis functions~$\vect\phi \coloneqq \{\varphi_1, \varphi_2, \dots, \varphi_\dimSize\}$ spanning~$V_h$, and with this we can represent $u_{h} = \sum_{i = 1}^{\dimSize} U_{i}\varphi_{i}$. Note that we only need to test that the equality~\eqref{finite} holds for each basis function, i.e.~\eqref{finite} is equivalent to
\begin{equation}\label{fem}
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{\dimSize} \text{ such that } \\
	\sum_{i = 1}^{\dimSize}\langle \mathcal{L}U_{i}\varphi_{i}, \varphi_{j} \rangle = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots,\dimSize.
\end{cases}
\end{equation}
If the differential equation~\eqref{pde} is linear, then~\eqref{fem} becomes a linear system
\[
\begin{cases} 
	\text{Find } U \in \mathbb{R}^{N} \text{ such that } \\
	\sum_{i = 1}^{N}\langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle U_{i} = \langle f, \varphi_{j} \rangle \; \forall j = 1, \ldots,\dimSize,
\end{cases}
\]
or
\begin{equation}\label{system}
	\vect A\,\vect x = \vect b
\end{equation}
with $\mathbf{A}_{i,j} = \langle \mathcal{L}\varphi_{i}, \varphi_{j} \rangle,\; \vect x_{i} = U_{i}$, and $\mathbf{b}_{i} = \langle f, \varphi_{i} \rangle$ for the right hand side. The \textit{finite element method} is a method for choosing these basis functions in such a way that the resulting system will have certain specified properties, e.g. one may require~$\mathbf{A}$ to be sparse, reasonably well-conditioned etc.

In the finite element method to choose the basis~$\vect\phi$, we first take the domain $\Omega$ and build its discrete representation~$\Omega_h$: We triangulate~$\Omega$ into a collection of polyhedral cells (or elements), typically tetrahedrons of hexahedrons. See Figure~\ref{fig:mesh}.

%One usually builds the basis~$\vect\phi$ using a concept of \textit{finite element}, see~\cite{ciarlet2002finite}. Roughly speaking, the finite element is a collection of 3 entities: Cell (typically a convex polygon in 2D or polyhedron 3D), finite dimensional space of shape functions~$S$ defined on this cell (typically polynomials of some degree~$k$), and the set of degrees of freedom (d.o.f.) for these shape functions (e.g. nodal values, mean values over edges or faces, normal or tangential components etc.) Given finite element, one then constructs a basis in~$S$ via associating a shape function with a d.o.f.: One fixes one degree of freedom to be unity and all the other d.o.f. to be zero for this shape function. 
%
%When the finite element is chosen, one constructs a trace (on a given cell) of the basis function~${\phi \in \vect\phi}$ from cell's shape function. This way it is guaranteed that each basis function has a compact support. See Figure~\AZ{Add.}

On each cell of~$\Omega_h$ we assign a set of points $p$ called degrees of freedom (d.o.f.); The basis functions are then chosen so that $\varphi_{i}(p_{j}) = \delta_{i}^{j}$, i.e. $i$th basis function is unity on its associated d.o.f. and is zero on all the other d.o.f. %This will have the effect of each basis function being a polynomial of degree $k$ with $k$ being the number of d.o.f. per cell.

\begin{figure}[H]
	\centering
	\includegraphicsw[.4]{mesh.png}
	\caption{Triangulation~$\Omega_h$ of the unit cube domain~$\Omega = (0, 1)^{3}$ consisting of hexahedral cells}\label{fig:mesh}
\end{figure}

With this particular choice, for cells that do not contain, say, the d.o.f. $p_{m}$, the corresponding basis function $\varphi_{m}$ will also be zero. This means that $\langle \mathcal{L}\varphi_{i}, \varphi_{m}\rangle = 0$ for any $i$ where $p_{i}$ is not contained in the same cell as $p_{m}$. More on this is in Section~\ref{sec:CSR}.

In this report we stick to \textit{Lagrange} (this means that d.o.f. are chosen to be nodal values) finite elements of polynomial degree~$K > 1$ defined on hexahedron elements, i.e. basis function $\varphi_i \in \vect\phi$ is a piecewise polynomial that is nonzero only in cells sharing vertex~$i$ of~$\Omega_h$. As we will see in Section~\ref{sec:mfree}, theoretically the matrix-free approach becomes more and more beneficial as one increases~$K$, i.e. for higher-order elements. 

In almost all applications, we use that 
$\langle \mathcal{L}\varphi_{i}, \varphi_{j}\rangle = \int_{\Omega}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. This makes assembly of the global system~\eqref{system} much simpler because we can write the integral as a sum over the cells $ \int_{\Omega}\mathcal{L}\varphi_{i}, \varphi_{j}dx = \sum_{\tau\in\Omega_h} \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx $. With this we now have what is called the cell (local) matrix 
$$ 
	\mathbf{A}^{cell}_{i,j} = \int_{\tau}\mathcal{L}\varphi_{i}\, \varphi_{j}dx\quad i, j = 1, \ldots k,
$$ 
with $k$ being the number of degrees of freedom per cell. In computing these local matrices, we then distribute them to the global matrix $ \mathbf{A} = \sum_{\tau}\vect P^{T}_{\tau}\mathbf{A}^{cell}\vect P_{\tau} $. Where $P_{tau}$ accounts for the mapping of global indices to local indices. The matrix assembly process is done cell-wise to compute the cell matrices that are then distributed to the global matrix~$\vect A$. 

One aspect that can make the local assembly process simpler is to assign the dofs to a reference cell and then use a change of coordinates to write the cell integral as an integral over the reference cell. This simply requires that we use the jacobian of the transformation to transform operations from the cell to the reference cell.

Once the matrix is assembled the linear system is then typically solved using an iterative method such as GMRES or Conjugate Gradient method. These methods are prefered due to only requiring matrix multiplications since when $\mathbf{A}$ is sparse, then the the complexity of a matrix-vector multiplication is only $O(n)$. 

This description of the finite element method has highlighted two key features: cell-wise matrix assembly and iterative linear solvers. As will be discussed below, these are the two key bottle necks that limit performance in most finite element codes which this project aims to address.

\subsection{Sparse matrix-vector multiplication}\label{sec:CSR}

A matrix $\vect B \in \mathbb R^{\dimSize\times\dimSize}$ is called \textit{sparse} iff $\sum_{i,\,j} \mbox{sign}\,\vect B_{i,j} = O(\dimSize)$ as $\dimSize \rightarrow \infty$, i.e. most of the elements are zero and need not to be stored. This way memory requirements are $O(\dimSize)$ vs. $\dimSize^2$ as in dense case. Given some requirements on the mesh~$\Omega_h$ and the choice of~$\vect\phi$ as explained in Section~\ref{sec:fem}, we have that our system matrix~$\vect A$ in~\eqref{system} is sparse, see Figure~\ref{fig:sparse}.

For big complex problems, especially in 3D, it is typical to use an iterative solver to solve~\eqref{system}. The problem may combine different variables (scalar and vector) and couple different physics (e.g. electromagnetics and fluid flow, fluid-structure interaction and so forth); In this case one would typically equip the iterative solver with an appropriate preconditioner (which may be rather complex) that takes the structure of the underline problem into account. In any case, both the solver and the application of the preconditioner are usually based on one core operation: \textit{Matrix-vector multiplication}
\begin{equation}\label{mv}
	\vect x \mapsto \vect A\,\vect x \eqqcolon \vect z.
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphicsw[.4]{sparse.pdf}
	\caption{Sparsity pattern of~$\vect A$}\label{fig:sparse}
\end{figure}

One uses special matrix formats to represent sparse matrices in a computer to achieve~$O(\dimSize)$ memory requirements. One popular choice is \textit{compressed sparse row} (CSR) storage format, see e.g.~\cite{saad2003iterative}. This format is quite economic in terms of memory requirements and is convenient to perform the matrix-vector multiplication:
\begin{lstlisting}[caption={Implementation of the sparse matrix-vector multiplication~\eqref{mv} for CSR format},label={lst:csrmv},captionpos=b]
	for (i = 0; i < nrows; ++i) 
		for (k = rowptr[i]; k < rowptr[i+1]; ++k)
			z[i] += val[k] * x[colind[k]];
\end{lstlisting}
Here the sparse matrix~$\vect A$ is represented with 3 vectors:
\begin{enumerate}
	\item Vector of values~\texttt{val} of size~$O(n)$ is the nonzero values of~$\vect A$ in row-by-row fashion,
	\item Vector of column indexes~\texttt{colind} with the same size as~\texttt{val} and with \texttt{colind[i]} being a column index of the element~\texttt{val[i]}, and 
	\item Vector of row pointers~\texttt{rowptr} of size $\dimSize + 1$ is a vector of indexes s.t. \texttt{rowptr[i]} is the starting index for the $i$th row of~$\vect A$, i.e. \texttt{val[rowptr[i]]} is the first nonzero element in the $i$th row and \texttt{rowptr[i + 1] - rowptr[i]} is the number of nonzero elements of $i$th row.
\end{enumerate}

CSR is optimal in the sense that both storage and multiplication require~$O(n)$ resources. However, one may note two major issues of the CSR multiplication implementation in Listing~\ref{lst:csrmv}:
\begin{enumerate}
	\item There is no spatial locality for~$\vect x$
	\item There are 5 memory refs vs. 2 arithmetic operations, i.e. the routine is memory bandwidth bounded.
\end{enumerate}
Moreover, the assembly of~$\vect A$ itself is a problem, for it requires explicit computation of the matrix elements (or elements of cell matrices). The alternative approach for implementing~\eqref{mv} that tries to deal with these issues is described in the next section.

\subsection{Matrix-free approach to matrix-vector multiplication}\label{sec:mfree}

In consideration of the bottlenecks and drawbacks presented above, there has been much research in how to avoid them; one such approach is known as the Matrix-Free finite element method. This method has technically been known for a long time~\cite{carey1988element}, it has only recently become popular with the advent of GPGPU computing~\cite{step37}. 

The idea of the matrix-free FEM is to avoid memory-related bottlenecks by avoiding computation fo the global matrix all together and take advantage of the cell-based assembly structure of the system. 

We begin with the cell based decomposition~$\mathbf{A} = \sum_{\tau}\vect P^{T}_{\tau}\mathbf{A}^{cell}\vect P_{\tau} $. We note that each row and column in the global matrix correspond to a single dof. Now we compute the matrix-vector product
\begin{align*}
\mathbf{A}x &= \left(\sum_{\tau}\vect P^{T}_{\tau}\mathbf{A}^{cell}\vect P_{\tau} \right)\vect x \\
				&= \sum_{\tau}\vect P^{T}_{\tau}\mathbf{A}^{cell}\vect P_{\tau}\vect x \\
				&= \sum_{\tau}\vect P^{T}_{\tau}\mathbf{A}^{cell}\vect x_{cell}.
\end{align*}
Here $\vect x_{cell} \in \mathbb{R}^{k}$ is a vector of values at the degrees of freedom from the given cell. This being know, we do not need to compute $P$ in practice, only keep track of the local to global numbering of the dofs. Now we compute $\vect y_{cell} = \mathbf{A}^{cell}\vect x_{cell}$ which then alows us to compute
\begin{align*}
\mathbf{A}\vect x &= \sum_{\tau}\vect P^{T}_{\tau}\vect y_{cell} \\
				&= \sum_{\tau} \vect y_{\tau}
\end{align*}
Now $\vect y_{\tau} \in \mathbb{R}^{N}$ is the vector with entries corresponding to contributions to the result from the given cell $\tau$. In practice, this can be done in one of two ways. One approach is to have distribute cells to various processors and have the processors compute the contribution from the locally owned cells and then to do a distributed reduction to gather the result into a single vector; this is method is what is typically used when using a distributed parallelization model such as MPI. The other approach is to assign each cell to a process and have the process compute the contributions from that cell and place the result directly into the resultant vector since the index mapping is known; this approach is more suitable for a shared memory approch such as OpenMP or Cuda\,/\,OpenACC. It is also possible to combine the two approaches when using a MPI + X parallelization model. In this project we use the approach of assigning each cell to a thread using OpenMP, more in Section~\ref{sec:impl}. For the remainder of this report we will be discussing the method using the OpenMP approach unless otherwise specified.

It is important to note that there are no approximations being made and that the result of a standard matrix-vector multiplication and application of this matrix-free approach will give the exact same result. The only difference is that for the matrix-free approach we compute the cell matrices ``on the fly'' as needed; as opposed to precomputing them and storing them in a global matrix. This will obviously require a significantly larger number of FLOPs, however the goal is to have a method that is CPU bounded and scales well as the number of processes is scaled up. The matrix free method ideally avoids memory speed limitations by not needing to access a global matrix at all. These benefits should become more visible as the matrix size become large and as the number of degrees of freedom per cell increases as well. For 2D problems with only a few degrees of freedom per cell the method is not expected to be as competitive due to the overhead associated with each cell. 

With the consideration that there will be a large overhead in computing the local matrix-vector product, there are a few optimizations that can be made. If for example we have that 
$$\mathbf{A}^{cell}_{i,j} = \int_{\tau}\nabla \alpha(x) \varphi_{i}(x) \cdot \nabla \varphi_{j}(x) dx\quad i,j = 1 \ldots \dofspercell$$ 
then we can do a change of coordinates to a reference cell
$$
	\mathbf{A}^{cell}_{i,j} = \int_{ref} \alpha(T(\hat{x})) (J^{-T}\nabla \varphi_{i}^{ref}(\hat{x})) \cdot (J^{-T}\nabla \varphi_{j}^{ref}(\hat{x}))\,\vert \mathrm{det}J(\hat x) \vert d\hat{x} \quad i,j = 1 \ldots \dofspercell.
$$ 
This can then be rewritten as 
\begin{equation}\label{decomp}
	\mathbf{A}^{cell} = \vect B^{T}_{ref}\,\vect J^{-1}_{cell}\,\vect D_{cell}\,\vect J^{-T}_{cell}\,\vect B_{ref}
\end{equation}
Where $\vect B_{ref}$ is a matrix of values of the gradient on the reference cell at each quadrature point, $\vect J$ is the Jacobian matrix of the transformation~$T$ from the reference cell to the physical cell~$\tau\in\Omega_h$, and $\vect D$ is a diagonal matrix with values of $\alpha(T(\hat{x}))\, \vert \mathrm{det}\vect J(\hat{x}) \vert$ along the diagonal. This is beneficial because the  values of $\vect B^{T}$ are the same for each cell and can be precomputed and shared.

Note that if we were building a global matrix, this matrix product would need to be (implicitly) computed in order to distribute it to the global matrix. This grows more time consuming as the number dofs per cell grows, or as the number of integration points grows (typically the number of quadrature points is dependent on~$\dofspercell$). However, if we simply need to evaluate $\vect A^{cell}\vect x_{cell}$, we can use the decomposition to compute a series of matrix-vector products instead of having to multiply all of these matrices together. This has the benefit of reducing work per cell by reducing the complexity per cell from $O(\dofspercell^{dim})$ to $O(\dofspercell)$; so we can expect that the matrix-free approach will be more competitive as~$\dofspercell$ grows.


\section{Implementation details}\label{sec:impl}

For this project we used the deal.II finite element library~\cite{dealII91} to handle the domain triangulation, dof enumeration, and the local to global index mapping. The two matrix multiplication methods were both implemented manually. The deal.II library has a tutorial on how to use their builtin matrix-free method; as well as a description of how the method is implemented internally. We used this description with modifications to implement our methods. In particular, the deal.II matrix-free builtin methods are designed around an MPI based parallelization, see~\cite{step37}; whereas we intended to use OpenMP based parallelization. We tested our implementation against deal.II's builtin matrix-vector multiplication to ensure that they give consistent output.

\subsection{Matrix-free}
In this section we discuss the matrix-free vmult function that will take an input vector and compute the product $\vect A\vect x$. In our implementation, this is a class member, so a number of variables relating to the reference cell have been precomputing in construction of the class.

Using the description above, we begin with a loop over each cell. On each cell, we use a single thread to generate an omp task to compute the contributions from that cell, then wait for each task to complete before returning from the function. This taskwait is important so that the cell iterator does not go out of scope before a task has completed. 
\begin{lstlisting}[caption={Matrix-free cell parallelization},,captionpos=b,label={lst:mfreepar}]
#pragma omp parallel
#pragma omp single
{
  	for(cell = dofh.begin_active(); cell != dofh.end(); cell++){
		#pragma omp task firstprivate(cell)
		{
			// cell computation
		}
	}
	#pragma omp taskwait
}            
\end{lstlisting}

We decided to use omp task because we are only able to iterate over deal.II's iterator type. Additionally, when the body of the loop is large, it can be beneficial to use an omp task as opposed to an omp parallel for. 

Then for each cell we do the following
\begin{enumerate}
\item Initialize quadrature points and weights, the inverse of the jacobian and its determinant
\item Initialize the local to global mapping
\item Precompute $\alpha(q)$ at each quadrature point
\item Extract input vector values at local dofs
\begin{lstlisting}
	Vector<double> cell_vec(dofs_per_cell);
	for (unsigned int i = 0; i<dofs_per_cell; ++i)
			cell_vec(i) = src(local_to_global[i]);
\end{lstlisting}
\item Apply $\vect B_{ref}$ to the cell vector; apply $\vect J^{-T}_{cell}$ to the result of previous multiplication; \dots ; apply $\vect B^{T}_{ref}$ to the result of previous multiplication, cf.~\eqref{decomp}
\item Distribute local result to global result vector
\begin{lstlisting}
for(unsigned int i=0; i<dofs_per_cell; i++){
#pragma omp atomic
	dst(local_to_global[i]) += cell_vec_dest(i);
}
\end{lstlisting}
\end{enumerate}
We have to use atomic to avoid race conditions in accessing the shared global result. This could be avoided in two ways, one is to apply a coloring scheme to each cell so that no task accesses the same dof at the same time, or each thread could have a private global vector and then would do a reduction to merge them all together. We did not choose the later approach since it would create a large memory footprint per thread. And in the former case, was outside the scope of our project.

\subsection{Explicit sparse matrix construction}

\texttt{Vmult} member function for CSR implementation takes the following form: 
\begin{lstlisting}[caption={CSR matrix multiplication with row-wise parallelization},captionpos=b]
void Vmult(const Vector<double> &src, Vector<double> &dst) const override {
dst = 0.;
#pragma omp parallel for
for (unsigned int i = 0; i < system_matrix.m(); ++i) {
auto val = system_matrix.begin(i);
for (unsigned int k = 0; k < sparsity_pattern.row_length(i); ++k, ++val) {
auto j = sparsity_pattern.column_number(i, k);
dst[i] += val->value() * src[j];
}
}
constraints.distribute(dst);
}
\end{lstlisting}

This is essentially implementation of Listing~\ref{lst:csrmv} that uses deal.II's interface for somewhat low-level access to~\texttt{system\_matrix} object through its sparsity pattern. Note that~\texttt{system\_matrix} itself has a method for multiplication, however, it does not use OpenMP; this is the reason we had to introduce the member function listed above.

In order to build~\texttt{system\_matrix}, we iterate through the mesh cells (as standard in finite element codes and deal.II in particular). The structure essentially repeats Listing~\ref{lst:mfreepar}. We refer to Section~\ref{sec:source} for the full source code. 

\section{Computational experiments}
In this report we consider the following problem, let $\Omega = (0, 1)^{dim}$ be the unit cube with triangulation $\Omega_h = \{\tau_{h}\}$.
\begin{align*}
&\text{Find } u_{h} \in V_h \subset H^{1}(\Omega) \text{ such that: }  \\
&\begin{cases}
\int_{\Omega} \alpha(x) \nabla u_{h} \cdot \nabla v_{h} dx  = \int_{\Omega} f(x) v_{h} dx \; \forall v_{h} \in V_h \\
\nabla u_{n} \cdot n = 0 \text{ on } \partial \Omega ,\qquad u_{h} \vert_{\tau_h} \in \mathbb{P}_{K}(\tau_h) 
\end{cases}
\end{align*}
Where $\mathbb{P}_{k}(\tau_h)$ is the set of polynomials of at most degree $K$. We take $\alpha(x) = \frac{1}{0.05 + 2\vert \vert x \vert \vert^{2}}$. Since we are only interested in the speed of the matrix multiplications and do not actually solve the full differential equation, we do not need to specify $f$.

For the triangulation we use $r$ global refinements of the unit cube and randomly perturb the vertices so that there is no possibility of reuse of data between cells, see Figure~\ref{fig:mesh}. For the choice of finite elements, we choose polynomials of at most degree $K$, $\mathcal{Q}_{K}$; for integration we use a Gauss Quadrature of degree $K+1$. 

We compared our implementation of the matrix-vector product using CSR format and the matrix-free version. The comparison was done by taking measurements on 5 sucessive applications of the matrix-vector multiplication.

For our experiments we varied $r$, $dim$, $K$, and the number of threads used $p$. We used perf to measure the number of cycles, instructions, cache references, and cache misses, as well as total run time. We also used omp\_get\_wtime() to specifically measure the time of the matrix-vector multiplication alone. In context, varying $r$ corresponds to increasing the number of cells; and increasing $dim$ and $K$ both increase the number of dofs per cell. We also used RAPL to compute the power draw on the processors.

We ran all these tests on the Stampede2 clusters using Intel Xeon Platinum 8160 ("Skylake") 2.1GHz processor. We did utilize hyper threading on up to three nodes with two sockets per node. We set OMP\_PROC\_BIND=close for thread allocation.

We also learned several limitation of profilers when trying to link with large libraries. TAU was unable to ever fully link with deal.II, hpctoolkit took an extrodinarly long time to do any sort of profiling, perfexpert was unable to measure FLOPs, and gprof was unable to provide FLOPs as well. We attempted to use PAPI directly to measure FLOPs but were unsucessful in getting the counters to work properly.~\includegraphics[width=12px]{clown.png}


\subsection{Results}
In the following figures, we use fixed value of $K = 3$ and $dim = 3$. The different refinement levels led to matrices of dimension $N =$ 2\,197, 15\,625, 117\,649, and 912\,673. With these values of $N, K, dim$ the sparse matrix will have 256 non-zero entries per row, with the total size of the CSR matrix data being 1.81, 14.17, 111.98, 890.27 MB. As far as the matrix-free method is concerned, the local matrix will have a dimension of 64x64 for a size of 32.77 KB per thread; the corresponding number of cells was 64, 512, 4\,096, 32\,768. We only present results for thread numbers up to 96 threads for reasons discussed in the analysis.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{clTime.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{mfTime.pdf}
	\end{subfigure}%
	\caption{Excecution time: CSR (left) and matrix-free (right) approaches}\label{fig:time}
\end{figure}

\begin{table}[H]
	\centering\caption{Best thread team sizes (see Figure~\ref{fig:time})}
	\includegraphicsw[.9]{bestTime.pdf}
\end{table}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{cache.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\linewidth}
		\includegraphicsw{inst.pdf}
	\end{subfigure}
	\caption{Cache miss ratio (left) and instructions per cycle (right) for the largest matrix size~${\dimSize = 912\,673}$}\label{fig:cache}
\end{figure}

\begin{table}[H]
	\centering\caption{Energy consumption for a single thread; TDP = 150\,W}\label{fig:rapl}
	\includegraphicsw[.9]{rapl.pdf}
\end{table}

\subsection{Analysis}
In Figure~\ref{fig:time}, we see that performance of the CSR multiplcation does not scale well w.r.t. the number of threads; we see that 48 threads preform better that 96. The underlying reason was explained in the end of Section~\ref{sec:CSR}.

With the matrix-free multiplication, we do see continual improvement with the number of threads, though there seem to be diminishing returns on increasing the number of threads. We believe this is due to the use of the atomic command at the end of each cell operation, with more threads there is a greater wait time for each thread to write to the global resultant vector. This could be avoided if a coloring scheme was applied to allow multiple threads to write at once. It is also observed that more threads show quite poor performance for a relatively small number of dofs, there will be a very small amount of work per thread with each thread having to wait for others giving very little speedup over a serial run. In any case, we still see that the matrix-free out preforms much better than the CSR implementation requiring roughly 50\% the time for large matrices, as expected.

In Figure~\ref{fig:cache}, we indeed see an improvement in cache-misses as well as better scaling with respect to the number of threads, one of the major goals of this project. Though we were not able to compute the number of FLOPs per second directly, we use the number of instructions per cycle as a proxy, to infer that we do generally see an improvement in performance with respect to the number of threads for the matrix-free approach. In comparison, we clearly see that the CSR multiplication achieves a maximum at 48 threads, not 96. For these reasons, we do not expect that our program is memory bounded and the syncronization problems discussed above serve as the major bottleneck for the matrix-free implementation. This could be verified by looking at the number of no-ops, as well as looking at how preformance increases if the atomic statement is removed (though we did not have time for these measurements).

Though there is not a significant difference in the power draw, we see that the matrix-free generally draws more power from the CPU and less power from the DRAM; this indicates that it is in fact not using the RAM as much, and is more reliant on the CPU. 

In our results we observed significant drop in preformance when using more than one node. This is clearly due to time spent waiting on network communication. In this case it would be much better to use a combination of MPI + OpenMP as discussed in Section~\ref{sec:mfree}. 

\section{Summary}
In summary, we have presented an implementation of the matrix-free finite element method using a shared memory parallelization scheme. We observe some improvements that allign with our expectations, while finding bottlenecks that should be avoided in the future.

Some improvements that could be made would be to use a coloring scheme to avoid use of any atomic statements. If this can be avoided then much better scaling could be expected and could be scaled to much larger number of threads using a GPU. Another approach would be to incorporate an MPI scheme and scale to more nodes and distribute cells to be locally owned by each core and use a reduction operation. In \cite{step37} they also apply SIMD vectorization in conjuction with MPI and report as much as 8 times speed up. 

\section*{Acknowledgement}
We would both like to thank Prof. Johnsson and Suyash Bakshi for their instruction through out the semester for a great course. We both learned a lot in topics that we specifically plan to use in our careers. 

\section{Source code}\label{sec:source}

The source code is available on github:~\url{https://github.com/elykwilliams/MatrixFree-FEM}. The contribution plot is available here:~\url{https://github.com/elykwilliams/MatrixFree-FEM/graphs/contributors}

\lstinputlisting[caption={MatrixFree.h}]{../MatrixFree.h}

\lstinputlisting[caption={main.cc}]{../main.cc}

\bibliographystyle{plain}
\bibliography{bibl}

\end{document}